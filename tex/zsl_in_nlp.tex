%%%%
%%%%
\documentclass[sigconf, review]{acmart}

%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% As of May 2020, [sigchi] and [sigchi-a] are no longer used. Please use sigconf (above) for SIGCHI conferences.

%%%% Proceedings format for SIGPLAN   conferences 
% \documentclass[sigplan, anonymous, authordraft]{acmart}

%%%% Proceedings format for conferences using one-column small layout
% \documentclass[acmsmall,authordraft]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{An Application Survey of Zero-Shot Learning on Natural Language Processing}

\author{Bowen Xu}
\affiliation{%
  \institution{Institute of Applied Electronics}
  \streetaddress{No.5 Yiheyuan Road Haidian District}
  \city{Beijing}
  \country{China}}
\email{xubowen@pku.edu.cn}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Zero-shot learning plays an important role facing the difficulty of laking annotated data. The technique or idea of zero-shot learning has been widely used in many traditional fields such as computer vision and natural language processing (NLP). While there are not any literatures to review the applications of zero-shot learning on NLP. In this paper, we make a comprehensive survey on that, aiming to give the researchers on zero-shot learning a full view of it. We first introduce the works of zero-shot learning on neural machine learning, natural language understanding, and knowledge graph constructing in a birds' eye view, and then detailly compare the methods and their performance in each application. 
\end{abstract}


% %%
% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% %% Please copy and paste the code instead of the example below.
% %%
% \begin{CCSXML}
%   <ccs2012>
%    <concept>
%     <concept_id>10010520.10010553.10010562</concept_id>
%     <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%     <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%     <concept_id>10010520.10010575.10010755</concept_id>
%     <concept_desc>Computer systems organization~Redundancy</concept_desc>
%     <concept_significance>300</concept_significance>
%    </concept>
%    <concept>
%     <concept_id>10010520.10010553.10010554</concept_id>
%     <concept_desc>Computer systems organization~Robotics</concept_desc>
%     <concept_significance>100</concept_significance>
%    </concept>
%    <concept>
%     <concept_id>10003033.10003083.10003095</concept_id>
%     <concept_desc>Networks~Network reliability</concept_desc>
%     <concept_significance>100</concept_significance>
%    </concept>
%   </ccs2012>
%   \end{CCSXML}
  
%   \ccsdesc[500]{Computer systems organization~Embedded systems}
%   \ccsdesc[300]{Computer systems organization~Redundancy}
%   \ccsdesc{Computer systems organization~Robotics}
%   \ccsdesc[100]{Networks~Network reliability}
  
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{zero-shot learning, neural machine translation, natural language understanding, knowledge graph}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Applications}
Zero-short Learning is a useful means when training data and respective labels are hard to achieve \cite{wang2019survey}. It has been widely combined with traditional neural methods of Computer Vision, Natural Language Processing (NLP), etc. In this paper, we make a comprehensive survey of the applications of zero-shot learning on NLP. We will introduce the application schemes and basic ideas of methods in the next few subsections. The applications in neural machine translation will be introduced more detailly, so that readers could comprehend the basic principles and methods of how to apply the zero-shot idea to some field. Then we will describe and compare the methods and their performances in each application in detail in section xx and section xx. 

\subsection{Neural Machine Translation}

Traditionally, neural machine translation (NMT) relies on large amounts of the parallel corpus. To translate a zero-shot source word to a target language, the basic assumption is that the mapping of representations between different linguistic words is linear. Similar semantics of different linguistic words under the same context have similar representations \cite{mikolov2013exploiting}. Based on that assumption, we can infer the representations, i.e. word embeddings as the terminology of NLP, of words unseen before if the words in similar semantic or with high relevance are learned by supervised learning. With the seen words, we can learn a mapping from the source language to the target language. Using the same mapping, the target linguistic representations of unseen words can be predicted well. 

A simpler problem than translation is dictionary induction, mapping words from a source language, as an entry, to equivalent words of a target language. Word embeddings of each language are learned, then given a seed dictionary a mapping is trained to connect the two linguistic vector spaces. A zero-hot word can be translated through the mapping \cite{nakashole2017knowledge}.

To consider the more complex problem, neural machine translation, which aims to translate a source sentence to a target one. The state-of-the-art deep-learning-based methods rely on big data. For the lack of parallel corpus, training becomes extremely hard. Zero-shot multilingual translation model assumes that the cross-lingual sentence pairs are rare in some languages. Although traditional pivot-based translation is used to address zero-shot translation, Firat et al. proposed a multilingual neural model to gain the ability of zero-shot learning, which is deficient in the traditional one-to-one and many-to-one translation strategies \cite{firat2016zero}. It is implicated that the zero-shot translation model can learn the common, underlying structures of multiple languages. Johnson et al. propose a more general method to translate multiple languages \cite{johnson2017google}. All the language-pairs are trained to learn shared parameters, which allows the models to generalize in the corpus-missing languages. Al-Shedivat et al. give the definition of zero-shot consistency to measure the feasibility of zero-shot learning \cite{al2019consistency}, and hereby take a different approach from a probabilistic perspective aiming to improve the training procedure of \cite{johnson2017google}. 

Zheng et al. alleviate the problem of error propagation in traditional segmentation decoding by maximizing the estimation of expectation likelihood with respect to the pivot-to-source model \cite{zheng2017maximum}. Chen et al. propose a teacher-student framework, in which a source-to-target translation model, as student, is trained without any parallel corpora \cite{chen2017teacher}. While another pivot-to-target translation model, as teacher, guides the student to learn on a source-pivot parallel corpus.

Zero-shot learning and dual learning are combined. Through dual learning, it gains better performance than solely using zero-shot learning \cite{sestorain2018zero,gu2018universal}. The basic idea of dual learning is that two agents with different models translate the original sentence to another language and then translate it back; by comparing the original and the translated back sentences both the models can improve their performance without any labels of data \cite{he2016dual}. Gu et al. propose a new translation approach that share lexical and sentence level representations of multiple languages \cite{gu2018universal}. A universal word-level representation space is shared by several languages, along with a sentence-level expert model assuming that sentences of different languages have similar structures.  A novel task of NMT presents a multi-agent setting in which agent learners engage in image description games with corporations.  Through the games, agents improve their own translation ability \cite{chen2018zero}. On account of the sensitivity of the hyper-parameter setting in the training stage, Guetal.capture the spurious correlations of mutual information between languages to improve the performance of the zero-shot model \cite{gu2019improved}.

\subsection{Natural Language Understanding}
Natural language understanding (NLU) aims to parse natural sentences into the form that a machine can use directly. For example, an NLU module transfers a sentence to a series of instructions to make some queries in a database. Leveraging the similarity between word representations of natural language and instruction representations of computer language, zero-shot learning can be easily achieved,i.e.training the word embeddings of the respective language, learning the mappings based on seed pairs, generalizing the mapping to unseen pairs is feasible \cite{ferreira2015zero, yazdani2015model}. When the training dataset for mapping an utterance to instruction is hard to build, the zero-shot NLU comes into play. Using domain-independent questions and sequential structure, it is showed that when trained in one specific domain(e.g.insurance), the system can also work in another specific domain (e.g.sightseeing) \cite{sadamitsu2017zero,bapna2017towards,herzig2018decoupling, lee2019zero}. Ferreira, Jabaian et al. \cite{ferreira2015online} and Ferreira, Masson, et al. \cite{ferreira2016adversarial} adopt an online adaptative strategy to refine the initial model progressively. Besides, an adversarial algorithm is used to learn the policy of online adaption \cite{ferreira2016adversarial}.

\subsection{Constructing Knowledge Graph}
To construct a knowledge graph, there are two steps. First, extract the entities from the natural sentences; second, extract relations between the entities. In both of the steps, zero-shot learning can help to extract knowledge when training data is limited. When extracting entities, there may not be enough pairs of the object and its entity label. Then the names of objects and entities can be mapped to embeddings so that the agent can connect any unseen words to embedding s of entities label \cite{pasupat2014zero,guerini2018toward}. Some more further problems like fine-grained entity typing \cite{ma2016label,obeidat2019description}, in which the several probable entities type should be recognized, and open entity classification \cite{zhou2019zero, zhang2020mzet}, in which newly defined entities can be identified flexibly, are explored. It should be emphasized that Zhang et al. adopts a memory augmented approach through which the knowledge from seen types can be transferred to the unseen ones \cite{zhang2020mzet}. Relation extraction methods with zero-shot learning are proposed \cite{levy2017zero}.To eliminate the restriction that the relations should all be seen before, Jia et al. propose a Parasitic Neural Network to extract the unseen relation though learning the general representations of relation types \cite{jia2020parasitic}.

In addition, some other problems related to constructing knowledge graphs are explored with zero-shot learning, such as entity linking \cite{rijhwani2019zero}, entity property recognition \cite{imrattanatrai2019identifying}, and so on \cite{rei2018zero,al2016recovering}.

\subsection{Other Sub-Fields of NLP}

With regard to the classification problem, the representations learned by NMT can be used for the downstream tasks. Eriguchi et al. propose a multilingual classifier which reuse the encoder learned by multilingual NMT to construct a task-specific class i fier \cite{eriguchi2018zero}. Other classification works refers to text classification \cite{dauphin2013zero,zhang2019integrating,dai2018multilingual}, documents’ topic classification \cite{song2019toward},intent classification \cite{xia2018zero,williams2019zero,chen2018zero}, etc. With regard to generation problem with zero-shot learning, the relative works are domain natural language generation \cite{dethlefs2017domain}, headline generation \cite{shen2018zero}, abstract generation \cite{duan2019zero}, question generation \cite{elsahar2018zero}, dialog generation \cite{zhao2018zero}, etc. Besides, other attempts with zero-shot learning, are made, such as cross-lingual slot filling \cite{shah2019robust}, cross-lingual document retrieval \cite{funaki2015image}, and cross-lingual word alignment \cite{schuster2019cross}.

% \section{Related Work}

% There are few similar works because zero-shot learning is a burgeoning field. Schuster et al. mainly review the settings and methods of zero-shot learning, with a few amounts of sentences to introduce the applications of zero-shot learning. Wang et al. review zero-shot learning mainly in terms of methods and settings \cite{wang2019survey}, while we mainly review it in terms of applications. We hope our comprehensive review of applications could help researchers to find innovation points more efficiently. 

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \bibliographystyle{ACM-Reference-Format}
\bibliographystyle{unsrt}
\bibliography{reference}

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
